link    --- https://kubernetes.io/docs/tasks/run-application/configure-pdb/


Pod Topology Spread Constraints == POD DISTRIUTUION CONTROL ON ZONE AND HOSTNAME(NODE)
              control how Pods are distributed across different failure domains (e.g., regions, zones, nodes) to enhance availability and resource utilization.

Key Components:
maxSkew: Defines the maximum allowed unevenness in Pod distribution. Must be greater than zero.
topologyKey: Specifies the topology domain (like zone or node) for spreading Pods.topology.   (   kubernetes.io/region   , topology.kubernetes.io/zone)
whenUnsatisfiable: Determines the action when constraints cannot be met (DoNotSchedule (strict so pending pod) | ScheduleAnyway (best-effort) Prod Choice: ScheduleAnyway).
labelSelector: Filters which Pods the constraints apply to based on labels.

scheduler sees all constraints(ZONE+NODE+RESOURCES AVAIL)i this as a single rules and finds a node that satisfies all of them at once.

Conditions it checks (numbered):

Zone Skew Check: (Pods_in_Zone_A - Pods_in_Zone_B) <= 1
Node Skew Check: (Pods_on_Node_1 - Pods_on_Node_2) <= 1
Resource Check: Node must have enough CPU/Memory for the pod.



######################################EXTRA IMP INFO
link---           https://youtu.be/ALI89evg0dY?si=75FLsHKF1wzJFcdL

apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone  # Distributes pods across zones for zone-level resilience.
    whenUnsatisfiable: DoNotSchedule          # Blocks scheduling if constraints can't be satisfied.
    labelSelector:
      matchLabels:
        app: foo                              # Targets pods labeled 'app: foo'.
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname       # Distributes pods across nodes for node-level resilience.
    whenUnsatisfiable: DoNotSchedule          # Ensures strict adherence to constraints.
    labelSelector:
      matchLabels:
        app: foo                              # Targets the same workload as the first constraint.




The scheduler considers both constraints simultaneously, only placing pods where both are satisfied (e.g., a pod must be placed in a specific zone and node).

If constraints cannot be met, scheduling is blocked (DoNotSchedule) until resources align with the rules.
