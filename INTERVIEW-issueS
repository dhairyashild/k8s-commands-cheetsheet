1 COREDNS --pod ask- coredns(/etc/resolve.conf)--coredns resolve svc-ns-S-C-L ---return IP to pod

2  2 POD CREATION LIFECYCLE
kubectl/user --- apiserver -- etcd --informer+watchloop---deployment controller triger reconcilation loop -cretae RS+pod 
--stores info etcd-- scheduler -- kubelet --

3 HPA CHECK 
Generate Artificial Load

4 Highly Available EKS Cluster — Key Steps (Short):
HPA
CLUSTER AUTOSCALER 
DEPLOY /RS
PDB

6 ROLLOUT ===HIISORY==UNDSTATUS
✅
 If still stuck==
 Check container images
 Verify ConfigMaps and Secrets
 Validate readiness and liveness probes
 Confirm RBAC and NetworkPolicies aren’t blocking traffic





#############################################################################
1 COREDNS

Step  What Happens
1  Pod/service created, DNS record auto-generated
2  Pod sends DNS queries to CoreDNS using the nameserver set in /etc/resolv.conf
3  Pod queries DNS for service/pod name
- Service DNS Naming Convention
<service-name>.<namespace>.svc.cluster.local
- Pod DNS Naming Convention=
<pod-ip-address>.<namespace>.pod.cluster.local
4  CoreDNS resolves name using Kubernetes API
5  IP address is returned to the querying pod
6  For external names, CoreDNS forwards to upstream




Interview Cheat Sheet
First Check: nslookup kubernetes.default (basic test).
If Fails check:
1. CoreDNS pods up?
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?

2. kube-dns Service IP correct?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns

3. VPC DNS enabled?
aws ec2 describe-vpcs --vpc-id <VPC-ID> --query "Vpcs[0].[EnableDnsHostnames, EnableDnsSupport]"

Advanced:
1. Check resolv.conf in failing pod. its auto-set in pods
nameserver 10.100.0.10                  # Pod sends DNS queries to CoreDNS using the nameserver set in /etc/resolv.conf
search namespace.svc.cluster.local ...   # Search domains
options ndots:5                         # Query behavior

Verify no NetworkPolicy blocks DNS.
kubectl get networkpolicies -A	                              #check Blocking UDP 53?

One-Liner to Test Everything:
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default.svc.cluster.local
This covers all critical DNS components in EKS concisely! 

#####################################################

2 POD CREATION LIFECYCLE


| Step | Component               | What It Does                                                                                      |
|------|------------------------|---------------------------------------------------------------------------------------------------|
| 1    | User/kubectl           | User creates a Deployment YAML or runs a kubectl command to define the desired state.             |
| 2    | API Server             | Receives the request and validates it.                                                            |
| 3    | etcd                   | API server stores the Deployment object and its configuration in etcd (the cluster’s key-value store). |
| 4    | Informer+Watch Loop    | Detects the new Deployment object via real-time updates from the API server.                      |
| 5    | Deployment Controller  | Reconciliation loop is triggered; compares desired vs. actual state.                              |
| 6    | Deployment Controller  | Creates or updates a ReplicaSet to match the desired number of pods.                              |
| 7    | etcd                   | Stores the new ReplicaSet object and its configuration.                                           |
| 8    | ReplicaSet Controller  | Ensures the correct number of pods exist by creating new pods as needed.                          |
| 9    | etcd                   | Stores new Pod objects and their configurations.                                                  |
| 10   | Scheduler              | Assigns new pods to appropriate nodes in the cluster.                                             |
| 11   | Kubelet                | On each node, pulls the container image and starts the pod.                                       |
| 12   | Controller Manager     | Continuously monitors and reconciles to maintain the desired state.                               |

Informer/Watch Loop: Detects changes in real time and triggers controllers.

Reconciliation: Controllers always work to match the cluster's actual state to the user’s desired state.

Event Handler: Each change (add, update, delete) triggers specific logic in the controllers to take action.
#################################################################

3 How would you simulate HPA behavior in staging without real traffic?

Generate Artificial Load
Run a load generator pod (such as BusyBox) to simulate traffic or resource usage. 
This repeatedly sends requests to your service, increasing CPU usage and triggering the HPA.

kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://<service-name>; done"

####################################

4
## Enforcing runtime security in Kubernetes requires multiple controls:

PodSecurityPolicy (PSP): Legacy, deprecated; do not rely on it for new clusters.

OPA/Gatekeeper & Kyverno: Use these admission controllers to enforce custom security policies on pod specs (e.g., block privileged containers, enforce image sources).

AppArmor/Seccomp: Apply kernel-level profiles to restrict container permissions and system calls at runtime.
                  AppArmor controls what files, programs, and system actions a container or pod can access or use on the node by applying security profiles at the kernel level.
1. Write and load the AppArmor profile on all nodes:
First, create your AppArmor profile and load it on each cluster node (using tools like apparmor_parser or with management tools).
2. Reference the profile in your Pod YAML using annotations:
apiVersion: v1
kind: Pod
metadata:
  name: hello-apparmor
  annotations:
    container.apparmor.security.beta.kubernetes.io/hello: localhost/k8s-apparmor-example-deny-write              ####
spec:


RBAC: Controls API access, but alone does not protect running pods from runtime threats.

Network Policies: Restrict pod-to-pod and external traffic to limit lateral movement.

Runtime monitoring (e.g., Falco): Detect and respond to suspicious activity in running containers.
##################################################################
5
Highly Available EKS Cluster — Key Steps (Short):

Multi-AZ Control Plane:
EKS automatically runs the control plane across multiple Availability Zones for resilience.

Distribute Worker Nodes:
Deploy worker nodes across at least three AZs to avoid single points of failure and ensure workload continuity.

Auto Scaling:
Use EC2 Auto Scaling groups or Cluster Autoscaler for dynamic node scaling and self-healing.

Pod Anti-Affinity:
Set pod anti-affinity rules to spread replicas across nodes and AZs for maximum redundancy.

Load Balancers:
Use AWS load balancers (ELB/ALB) to distribute traffic across healthy nodes and AZs.

Backup and Disaster Recovery:
Regularly back up etcd and have a disaster recovery plan for cluster state and workloads.

Monitoring:
By prom and grafana
#################################################################
6 Can You Fix a Stuck Rollout with kubectl rollout undo?

































































































