/16	65,536
/19	8,192
/24	256
1 COREDNS --pod ask- coredns(/etc/resolve.conf)--coredns resolve svc-ns-S-C-L ---return IP to pod

2  2 POD CREATION LIFECYCLE
kubectl/user --- apiserver -- etcd --informer+watchloop---deployment controller triger reconcilation loop -cretae RS+pod 
--stores info etcd-- scheduler -- kubelet --

3 HPA CHECK 
Generate Artificial Load

4 Highly Available EKS Cluster — Key Steps (Short):
HPA
CLUSTER AUTOSCALER 
DEPLOY /RS
PDB

6 ROLLOUT ===HIISORY==UNDSTATUS
✅
 If still stuck==
 Check container images
 Verify ConfigMaps and Secrets
 Validate readiness and liveness probes
 Confirm RBAC and NetworkPolicies aren’t blocking traffic





#############################################################################
1 COREDNS

Step  What Happens
1  Pod/service created, DNS record auto-generated
2  Pod sends DNS queries to CoreDNS using the nameserver set in /etc/resolv.conf
3  Pod queries DNS for service/pod name
- Service DNS Naming Convention
<service-name>.<namespace>.svc.cluster.local
- Pod DNS Naming Convention=
<pod-ip-address>.<namespace>.pod.cluster.local
4  CoreDNS resolves name using Kubernetes API
5  IP address is returned to the querying pod
6  For external names, CoreDNS forwards to upstream




Interview Cheat Sheet
First Check: nslookup kubernetes.default (basic test).
If Fails check:
1. CoreDNS pods up?
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?

2. kube-dns Service IP correct?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns

3. VPC DNS enabled?
aws ec2 describe-vpcs --vpc-id <VPC-ID> --query "Vpcs[0].[EnableDnsHostnames, EnableDnsSupport]"

Advanced:
1. Check resolv.conf in failing pod. its auto-set in pods
nameserver 10.100.0.10                  # Pod sends DNS queries to CoreDNS using the nameserver set in /etc/resolv.conf
search namespace.svc.cluster.local ...   # Search domains
options ndots:5                         # Query behavior

Verify no NetworkPolicy blocks DNS.
kubectl get networkpolicies -A	                              #check Blocking UDP 53?

One-Liner to Test Everything:
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default.svc.cluster.local
This covers all critical DNS components in EKS concisely! 

#####################################################

2 POD CREATION LIFECYCLE


| Step | Component               | What It Does                                                                                      |
|------|------------------------|---------------------------------------------------------------------------------------------------|
| 1    | User/kubectl           | User creates a Deployment YAML or runs a kubectl command to define the desired state.             |
| 2    | API Server             | Receives the request and validates it.                                                            |
| 3    | etcd                   | API server stores the Deployment object and its configuration in etcd (the cluster’s key-value store). |
| 4    | Informer+Watch Loop    | Detects the new Deployment object via real-time updates from the API server.                      |
| 5    | Deployment Controller  | Reconciliation loop is triggered; compares desired vs. actual state.                              |
| 6    | Deployment Controller  | Creates or updates a ReplicaSet to match the desired number of pods.                              |
| 7    | etcd                   | Stores the new ReplicaSet object and its configuration.                                           |
| 8    | ReplicaSet Controller  | Ensures the correct number of pods exist by creating new pods as needed.                          |
| 9    | etcd                   | Stores new Pod objects and their configurations.                                                  |
| 10   | Scheduler              | Assigns new pods to appropriate nodes in the cluster.                                             |
| 11   | Kubelet                | On each node, pulls the container image and starts the pod.                                       |
| 12   | Controller Manager     | Continuously monitors and reconciles to maintain the desired state.                               |

Informer/Watch Loop: Detects changes in real time and triggers controllers.

Reconciliation: Controllers always work to match the cluster's actual state to the user’s desired state.

Event Handler: Each change (add, update, delete) triggers specific logic in the controllers to take action.
#################################################################

3 How would you simulate HPA behavior in staging without real traffic?

Generate Artificial Load
Run a load generator pod (such as BusyBox) to simulate traffic or resource usage. 
This repeatedly sends requests to your service, increasing CPU usage and triggering the HPA.

kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://<service-name>; done"

####################################

4
## Enforcing runtime security in Kubernetes requires multiple controls:

PodSecurityPolicy (PSP): Legacy, deprecated; do not rely on it for new clusters.

OPA/Gatekeeper & Kyverno: Use these admission controllers to enforce custom security policies on pod specs (e.g., block privileged containers, enforce image sources).

AppArmor/Seccomp: Apply kernel-level profiles to restrict container permissions and system calls at runtime.
                  AppArmor controls what files, programs, and system actions a container or pod can access or use on the node by applying security profiles at the kernel level.
1. Write and load the AppArmor profile on all nodes:
First, create your AppArmor profile and load it on each cluster node (using tools like apparmor_parser or with management tools).
2. Reference the profile in your Pod YAML using annotations:
apiVersion: v1
kind: Pod
metadata:
  name: hello-apparmor
  annotations:
    container.apparmor.security.beta.kubernetes.io/hello: localhost/k8s-apparmor-example-deny-write              ####
spec:


RBAC: Controls API access, but alone does not protect running pods from runtime threats.

Network Policies: Restrict pod-to-pod and external traffic to limit lateral movement.

Runtime monitoring (e.g., Falco): Detect and respond to suspicious activity in running containers.
##################################################################
5
Highly Available EKS Cluster — Key Steps (Short):

Multi-AZ Control Plane:
EKS automatically runs the control plane across multiple Availability Zones for resilience.

Distribute Worker Nodes:
Deploy worker nodes across at least three AZs to avoid single points of failure and ensure workload continuity.

Auto Scaling:
Use EC2 Auto Scaling groups or Cluster Autoscaler for dynamic node scaling and self-healing.

Pod Anti-Affinity:
Set pod anti-affinity rules to spread replicas across nodes and AZs for maximum redundancy.

Load Balancers:
Use AWS load balancers (ELB/ALB) to distribute traffic across healthy nodes and AZs.

Backup and Disaster Recovery:
Regularly back up etcd and have a disaster recovery plan for cluster state and workloads.

Monitoring:
By prom and grafana
#################################################################
6 Can You Fix a Stuck Rollout with kubectl rollout undo?




Expertise in implementing and managing monitoring solutions using Grafana and Prometheus?
############################################################################

. What if Your Service Works in Staging, but Not Production?give solution for eks stepwise and at last give stepwise summery also
3. What if Your Service Works in Staging, but Not Production?give solution for eks stepwise and at last give stepwise summery also
What If Your Service Works in Staging, but Not in Production? (EKS Solution)
When your service works in staging but fails in production on Amazon EKS, follow these stepwise troubleshooting and solution steps:

Stepwise Troubleshooting for EKS
1. Confirm the Problem
Identify the specific symptoms (timeouts, 5xx errors, connectivity issues).

Check if the problem is consistent or intermittent, and which users or endpoints are affected.

2. Compare Staging and Production Environments
Review Kubernetes versions, node types, resource limits, and configuration differences between staging and production.

Ensure that both environments have matching configurations for critical components (e.g., environment variables, secrets, config maps, IAM roles).

3. Check Service and Endpoint Health
Use:

kubectl get svc <service-name>

kubectl get endpoints <service-name>

Ensure all expected pods are registered as healthy endpoints in production.

4. Validate Networking and Security
Examine VPC, subnet, and security group settings for production.

Confirm that required ports are open and network policies permit necessary traffic.

Check for differences in ingress/egress rules between environments.

5. Inspect Load Balancer and DNS
Verify that the AWS load balancer is correctly configured and targets healthy pods.

Ensure DNS records resolve correctly to the production service.

6. Review Pod and Node Logs
Use:

kubectl describe pod <pod-name>

kubectl logs <pod-name>

Look for errors, restarts, or readiness/liveness probe failures in production pods.

7. Check IAM Roles and Permissions
Ensure the production environment has the correct IAM roles and policies for accessing AWS resources (e.g., S3, RDS, ECR).

8. Resource Limits and Scaling
Compare CPU, memory, and pod quotas between staging and production.

Check for resource exhaustion or scheduling failures in production.

9. Validate External Dependencies
Confirm that production services can reach required external APIs, databases, or third-party services.

Check secrets and credentials for correctness.

10. Monitor and Collect Metrics
Use AWS CloudWatch, Prometheus, or other monitoring tools to compare metrics between environments.

Look for anomalies in latency, error rates, or resource usage.

11. Test with Staging Traffic in Production
If possible, replay staging traffic patterns in production (with caution) to reproduce the issue.

12. Rollback or Hotfix
If the issue is critical and cannot be quickly resolved, consider rolling back to a previous stable deployment in production.



############################################################################
5. What if a Node Goes NotReady, but the Kubelet Logs Are 
Clean?
 This is one of the most misleading situations:
 node status is NotReady
 but journalctl -u kubelet shows nothing alarming


✅
 Inspect node conditions
 kubectl describe node <node>
 look for DiskPressure
 MemoryPressure
 NetworkUnavailable
 ✅
 Check Kubernetes events
 kubectl get events --field-selector involvedObject.kind=Node
 ✅
 Check container runtime
 ✅
 journalctl -u containerd
 docker info if Docker
 check disk usage in /var/lib/kubelet
 Inspect CNI plugin
 Logs for CNI pods
 bridge interfaces on the node
 ✅
 Check kube-proxy
 kubectl logs -n kube-system <kube-proxy-pod>
 validate iptables rules
 ✅
 Node-level resource issues
 top for load
 df -h for disk
 dmesg for kernel events
 ✅
 Cloud-specific
 hypervisor maintenance
 autoscaler scale-downs
 cloud provider instance status
 ✅
 Remediate
 cordon the node: kubectl cordon <node>
 drain:
 bash
 CopyEdit
 kubectl drain <node> --ignore-daemonsets --delete-emptydir-data
 replace or investigate offline
 NotReady doesn’t always mean kubelet is broken — think bigger: runtime, network, or even the 
cloud node itself


############################################################################

EKS vs. ECS: 3 Most Important Differences
Orchestration
Runs Kubernetes (open-source, industry standard, portable)	
Uses AWS’s proprietary orchestration engine

Flexibility
supports multi-cloud, 
deeply integrated with AWS

Cost & Complexity
More complex to operate; incurs extra control plane cost ($70/mo)	Easier to use, no control plane cost, faster to start

############################################################################
MEMORY

High MEMORY Usage/OOM Errors:
-leading to slow performance or 
-crashes when memory is exhausted.

SOLUTION :
Tune JVM Options:
-Adjust Java Virtual Machine (JVM) memory settings (like -Xmx, -Xms) and 
-garbage collection parameters to optimize resource usage.

############################################################################
CPU

-causing application slowdowns or unresponsiveness.

SOLUTION :

-kubectl top pods, kubectl describe pod== to verify real-time CPU usage
-Optimize inefficient code, algorithms, or thread usage to reduce unnecessary CPU consumption.
-Scale up resources by increasing CPU limits/requests or adding more replicas if the load is legitimate and sustained.
-Review deployment and resource configurations in version control and CI/CD to ensure changes are tracked and tested before production rollout
-Set up Grafana alerts for CPU thresholds to notify the team of future spikes.

############################################################################



kubeconfig--

Cluster field: Contains Kubernetes API server URL and cluster info.
Context field: Connects user, cluster, and namespace settings.
Users field: Stores authentication details like certificates or tokens.

































































