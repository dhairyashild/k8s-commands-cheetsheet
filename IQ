| Key Point          | Amazon EKS                     | Self-Managed Kubernetes          |
|--------------------|-------------------------------|---------------------------------|
| Management         | 🟢 Fully managed by AWS        | 🟡 User manages everything      |
| Updates & Patching | 🟢 Automatic security patches & upgrades | 🟡 Manual setup required |
| High Availability  | 🟢 Built-in multi-AZ support   | 🟡 Manual AUTOSCALING needed    |
| Control            | 🟡 Less control, more simplicity | 🟢 Full control, more complexity |

🔗 VPC Peering: Creates a direct network connection between two VPCs for private communication without using the internet.

🛠️ kubectl exec: Run commands inside a running container (open shell to debug).
📜 kubectl logs: View logs for troubleshooting output and errors.
🔎 kubectl describe: Get info and events about a resource for debugging issues.

❓ Q: main difference why ecs made if eks there..==ECS was designed to provide simpler container orchestration tightly integrated with AWS and eks for multicloud

📊 Complexity:
- 🟢 ECS is simpler to manage, ideal for AWS-integrated applications.
- 🔵 EKS supports complex microservices architectures with Kubernetes flexibility.

🌍 Portability:
- 🔴 ECS ties workloads to AWS, limiting portability.
- 🟢 EKS offers multi-cloud compatibility via Kubernetes' open standards

❓ Q: how to choose deploy strategy
✅ on company need
🔵🔵 bluegreen & 🐦 canary for 0 downtime but more resources
🔄 rolling for 0 downtime 

🔥 I've interviewed 100+ DevOps engineers. Most say "I've done K8s in production."
But when real questions come, they freeze.

🚨 Here's what I ask:

1. 🔍 How does DNS resolution work inside a pod?
   → And what do you check when a service isn't reachable by name?

2. ⚙️ Walk me through what the controller manager does during a Deployment.
   → Not rollout status. Reconciliation logic.

3. 💾 What happens if a node with local storage gets autoscaled down?
   → Be careful. This causes data loss in prod more often than you'd think.

4. 📈 Post-deploy, latency spikes for 30% of users. No errors. No logs. What now?
   → Your answer reveals if you know how to triage chaos.

5. 🔒 How do you enforce runtime security in Kubernetes?
   → PSP? AppArmor? OPA? Most people just hope for the best.

6. 📊 HPA vs VPA vs Karpenter — when would you NOT use each?
   → Bonus: How simulate HPA behavior in staging?

7. 🚨 Tell me about last outage you debugged in Kubernetes.
   → No postmortem? You weren't really there.

❓ "A pod can't resolve service names. DNS looks fine. What's next move?"

🛠️ 2. Test DNS Resolution from Debug Pod
```bash
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default


❓  NetworkPolicy debugging steps

1. kubectl describe networkpolicy
Shows policy details (podSelector, rules); error if policy doesn't exist.

2. kubectl get networkpolicy
Lists active policies in namespace; error if none found.

3. kubectl get pods --show-labels
Displays pod labels; error if labels don’t match policy selector.

4. Default allow or deny?
Checks if traffic is allowed by default; error if default-deny blocks traffic.

5. Default-deny policy exists?
Confirms explicit block policy; error if missing intended isolation.

6. nicolaka/netshoot image
Tests pod connectivity; error if traffic blocked (network issue).

7. CNI provider logs
Shows policy enforcement errors; error if CNI denies traffic.

8. Cloud Security Groups (AWS)
Reveals conflicting security rules; error if SG overrides NetworkPolicy.






• Debugging stuck pods & taints/tolerations 1 LINE EACH

kubectl describe pod (check Events for errors)
kubectl logs pod (check container logs)
kubectl get nodes (check node status)
Check resource quotas (quota exceeded)

Taints/Tolerations:

kubectl describe node (check node taints)
Check pod tolerations (yaml match)
Check node selector (node affinity)
Check pod priority (preemption)
Check custom scheduler (scheduling conflict)



1. How do you handle secrets in Kubernetes securely?
Answer:
• Use Kubernetes Secrets (base64 encoded, not encrypted by default).
• Enable Encryption at Rest for secrets in etcd.
• Use external secret providers (HashiCorp Vault, AWS Secrets Manager, Azure Key Vault).
• Restrict access with RBAC and least-privilege principles.

⸻

2. How do you debug a pod stuck in Pending state?
Answer:
• Run: `kubectl describe pod <name>` to see events.
• Check for: insufficient resources, node selector/taint issues, or unbound PVCs.
• Use: `kubectl get events --sort-by=.metadata.creationTimestamp`

⸻

3. How do you achieve multi-tenancy in Kubernetes?
Answer:
• Isolate teams/workloads using Namespaces.
• Apply ResourceQuotas and LimitRanges per namespace.
• Use NetworkPolicies for network isolation.
• Enforce RBAC for access control.
• Use policy engines (OPA Gatekeeper, Kyverno) for governance.

⸻

4. How would you implement canary deployments in Kubernetes?
Answer:
• Create two Deployments (stable and canary).
• Use a Service with selectors for both.
• Control traffic split via Service Mesh (Istio) or Ingress (Nginx).
• Gradually increase canary traffic; roll back on failure.

⸻

5. How do you secure communication between microservices?
Answer:
• Use a service mesh (Istio, Linkerd) for mTLS.
• Apply NetworkPolicies to restrict pod-to-pod traffic.
• Use cert-manager for automatic TLS certificate management.

⸻

6. How do you scale Kubernetes clusters automatically?
Answer:
• Horizontal Pod Autoscaler (HPA) for scaling pods.
• Cluster Autoscaler for adding/removing nodes.
• Vertical Pod Autoscaler (VPA) for adjusting resource requests/limits.

⸻

7. How do you manage manifests across environments?
Answer:
• Use Kustomize with overlays (base/, dev/, prod/).
• Use Helm with environment-specific values.yaml.
• Adopt GitOps (ArgoCD, Flux) for continuous deployment.

⸻

8. How do you ensure zero downtime during upgrades?
Answer:
• Use RollingUpdate strategy in Deployments.
• Configure PodDisruptionBudgets (PDBs).
• Use readiness probes to ensure traffic only goes to healthy pods.
• Upgrade nodes gradually (drain/cordon).

⸻

9. How do you enforce security policies cluster-wide?
Answer:
• Use Pod Security Admission (PSA) for pod security.
• Implement RBAC for authorization.
• Use OPA Gatekeeper or Kyverno for custom policies.
• Scan images for vulnerabilities (Trivy, Clair).
• Enable API server audit logging.

⸻

Traffic spike =Causes CPU/memory/network metrics to go up

CPU spike: Check pod/node usage for throttling or overload.
Memory spike: Check for high pod/node usage, evictions, OOM kills.
Network spike: Check traffic in/out, DDoS, backups, data syncs.

Check Prometheus/Grafana dashboards for real-time CPU, memory, network spikes.

#SOLUTION===
1) real traffic increase  == then autoscale
2) hacker attack == block bad traffic (WAF=RATE-LIMIT + SG )
3) Optimize code by devolopers if there is memory leak
⸻

NACL ISSUE ==
pending pod --- node not ready ---documentation gives 3 ports must open for autoscale

⸻
500: Internal Server Error (App Crash)
CRASHLOOP: kubectl describe pod + kubectl logs
OOMKilled: Autoscale pod memory
ENV VAR ERROR: Check configmap/secret linked to pod
LIVENESS FAIL: Check probe path & port

502: Bad Gateway  (Networking Issue)
ROOT CAUSE: Network path from ALB to pod broken.
CHECK: kubectl get endpoints - if empty, fix:
Service selector labels
Pod label matches service
Pod readiness (must be 1/1)

503: Service Unavailable (No Ready Pods) == READINESS PROBE problem only  
ROOT CAUSE: Pods are running but not ready.
CHECK: kubectl get pods → Look for 0/1 READY
FIX: Increase initialDelaySeconds or fix  /health endpoint Readiness probe 

504: Gateway Timeout == (Slow Response)
ROOT CAUSE: App response > ALB timeout (60s default).
CHECK:
kubectl top pods → Is pod CPU throttled?
App logs → Slow queries/hangs?
FIX: Optimize app OR increase ALB idle timeout. 

⸻


⸻


⸻


⸻



⸻



⸻



