Q main difference why ecs made if eks there..==ECS was designed to provide simpler container orchestration tightly integrated with AWS and eks for multicloud
Complexity:

ECS is simpler to manage, ideal for AWS-integrated applications.
EKS supports complex microservices architectures with Kubernetes flexibility.

Portability:
ECS ties workloads to AWS, limiting portability.
EKS offers multi-cloud compatibility via Kubernetes' open standards



Q   how to choose deploy strategy

on company need
bluegreen & canary for 0 downtime but more resources
rolling for 0 downtimetime 


 I’ve interviewed 100+ DevOps engineers. Most of them say, “I’ve done K8s in production.”
But when the real questions come in, they freeze.

Here’s what I ask 👇

🔥 1. How does DNS resolution work inside a pod?
→ And what do you check when a service isn’t reachable by name?

🔥 2. Walk me through what the controller manager does during a Deployment.
→ Not rollout status. Reconciliation logic.

🔥 3. What happens if a node with local storage gets autoscaled down?
→ Be careful. This one causes data loss in prod more often than you’d think.

🔥 4. Post-deploy, latency spikes for 30% of users. No errors. No logs. What now?
→ Your answer reveals if you know how to triage chaos.

🔥 5. How do you enforce runtime security in Kubernetes?
→ PSP? AppArmor? OPA? Most people just hope for the best.

🔥 6. HPA vs VPA vs Karpenter — when would you NOT use each?
→ Bonus: How would you simulate HPA behavior in staging?

🔥 7. Tell me about the last outage you debugged in Kubernetes.
→ No postmortem? You weren’t really there.



“A pod can’t resolve service names. DNS looks fine. What’s your next move?”


2. Test DNS Resolution from a Debug Pod
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default
Why?=kubernetes is basic svc always run on k8s, kubernetes.default=kubernetes.default.svc.cluster.local , no need last words if in same ns
Confirms if basic DNS resolution works inside a pod.
If this fails, CoreDNS or networking is misconfigured.

Key Troubleshooting Scenarios
If nslookup kubernetes.default fails but nslookup kubernetes.default.svc.cluster.local works:
→ The pod's search domains in /etc/resolv.conf are misconfigured

If both fail:
→ CoreDNS is down or network policies block DNS
1. Verify CoreDNS 2+ Pods running or not
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns
=Ensures CoreDNS pods are running.
 If not, DNS resolution fails.
k logs and other commands to check , last resort delete coredns deploy & re-install






Interview Cheat Sheet
First Check: nslookup kubernetes.default (basic test).
If Fails check:
1. CoreDNS pods up?
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?

2. kube-dns Service IP correct?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns

3. VPC DNS enabled?
aws ec2 describe-vpcs --vpc-id <VPC-ID> --query "Vpcs[0].[EnableDnsHostnames, EnableDnsSupport]"

Advanced:
1. Check resolv.conf in failing pod. its auto-set in pods
nameserver 10.100.0.10                  # CoreDNS ClusterIP
search namespace.svc.cluster.local ...   # Search domains
options ndots:5                         # Query behavior

Verify no NetworkPolicy blocks DNS.
kubectl get networkpolicies -A	                              #check Blocking UDP 53?

One-Liner to Test Everything:
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default.svc.cluster.local
This covers all critical DNS components in EKS concisely! 


kubeconfig--

Cluster field: Contains Kubernetes API server URL and cluster info.
Context field: Connects user, cluster, and namespace settings.
Users field: Stores authentication details like certificates or tokens.




Deployment strategies you should know as a Devops Engineer:

1. Recreate
- delete all instances then add all instance 
- downtime

2. Rolling update
- replace gradually 
- zero downtime 
- production based app

3. Blue green deployment 
- 2 environments are created
- blue - old instance, green - new instance 
- no downtime 
- high cost
- high level of testing is required

4. Canary deployment 
- 2 environments are created
- a small number of users migrated to a new app 
- no downtime 
- high cost
- high level of testing is required 

