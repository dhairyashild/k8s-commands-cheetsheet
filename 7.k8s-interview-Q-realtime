| Key Point          | Amazon EKS                     | Self-Managed Kubernetes          |
|--------------------|-------------------------------|---------------------------------|
| Management         | Fully managed by AWS           | User manages everything          |
| Updates & Patching | Automatic security patches&upgrades| Manual setup required        |
| High Availability  | Built-in multi-AZ support      | Manual AUTOSCALING needed        |
| Control            | Less control, more simplicity  | Full control, more complexity   |

VPC Peering: Creates a direct network connection between two VPCs for private communication without using the internet.

kubectl exec: Run commands inside a running container (open shell to debug).
kubectl logs: View logs for troubleshooting output and errors.
kubectl describe: Get info and events about a resource for debugging issues.

Q main difference why ecs made if eks there..==ECS was designed to provide simpler container orchestration tightly integrated with AWS and eks for multicloud
Complexity:

ECS is simpler to manage, ideal for AWS-integrated applications.
EKS supports complex microservices architectures with Kubernetes flexibility.

Portability:
ECS ties workloads to AWS, limiting portability.
EKS offers multi-cloud compatibility via Kubernetes' open standards



Q   how to choose deploy strategy

on company need
bluegreen & canary for 0 downtime but more resources
rolling for 0 downtimetime 


 I‚Äôve interviewed 100+ DevOps engineers. Most of them say, ‚ÄúI‚Äôve done K8s in production.‚Äù
But when the real questions come in, they freeze.

Here‚Äôs what I ask üëá

üî• 1. How does DNS resolution work inside a pod?
‚Üí And what do you check when a service isn‚Äôt reachable by name?

üî• 2. Walk me through what the controller manager does during a Deployment.
‚Üí Not rollout status. Reconciliation logic.

üî• 3. What happens if a node with local storage gets autoscaled down?
‚Üí Be careful. This one causes data loss in prod more often than you‚Äôd think.

üî• 4. Post-deploy, latency spikes for 30% of users. No errors. No logs. What now?
‚Üí Your answer reveals if you know how to triage chaos.

üî• 5. How do you enforce runtime security in Kubernetes?
‚Üí PSP? AppArmor? OPA? Most people just hope for the best.

üî• 6. HPA vs VPA vs Karpenter ‚Äî when would you NOT use each?
‚Üí Bonus: How would you simulate HPA behavior in staging?

üî• 7. Tell me about the last outage you debugged in Kubernetes.
‚Üí No postmortem? You weren‚Äôt really there.



‚ÄúA pod can‚Äôt resolve service names. DNS looks fine. What‚Äôs your next move?‚Äù


2. Test DNS Resolution from a Debug Pod
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default
Why?=kubernetes is basic svc always run on k8s, kubernetes.default=kubernetes.default.svc.cluster.local , no need last words if in same ns
Confirms if basic DNS resolution works inside a pod.
If this fails, CoreDNS or networking is misconfigured.

Key Troubleshooting Scenarios
If nslookup kubernetes.default fails but nslookup kubernetes.default.svc.cluster.local works:
‚Üí The pod's search domains in /etc/resolv.conf are misconfigured

If both fail:
‚Üí CoreDNS is down or network policies block DNS
1. Verify CoreDNS 2+ Pods running or not
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns
=Ensures CoreDNS pods are running.
 If not, DNS resolution fails.
k logs and other commands to check , last resort delete coredns deploy & re-install






Interview Cheat Sheet
First Check: nslookup kubernetes.default (basic test).
If Fails check:
1. CoreDNS pods up?
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?

2. kube-dns Service IP correct?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns

3. VPC DNS enabled?
aws ec2 describe-vpcs --vpc-id <VPC-ID> --query "Vpcs[0].[EnableDnsHostnames, EnableDnsSupport]"

Advanced:
1. Check resolv.conf in failing pod. its auto-set in pods
nameserver 10.100.0.10                  # CoreDNS ClusterIP
search namespace.svc.cluster.local ...   # Search domains
options ndots:5                         # Query behavior

Verify no NetworkPolicy blocks DNS.
kubectl get networkpolicies -A	                              #check Blocking UDP 53?

One-Liner to Test Everything:
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default.svc.cluster.local
This covers all critical DNS components in EKS concisely! 


kubeconfig--
Cluster field: Contains Kubernetes API server URL and cluster info.
Context field: Connects user, cluster, and namespace settings.
Users field: Stores authentication details like certificates or tokens.




Deployment strategies you should know as a Devops Engineer:

1. Recreate
- delete all instances then add all instance 
- downtime

2. Rolling update
- replace gradually 
- zero downtime 
- production based app

3. Blue green deployment 
- 2 environments are created
- blue - old instance, green - new instance 
- no downtime 
- high cost
- high level of testing is required

4. Canary deployment 
- 2 environments are created
- a small number of users migrated to a new app 
- no downtime 
- high cost
- high level of testing is required 





‚Ä¢ Use case for ConfigMap vs Secret
---
# ConfigMap: Non-sensitive application config for Java Spring Boot backend on AWS EKS
apiVersion: v1
kind: ConfigMap
metadata:
  name: prod-app-config
  namespace: production # Isolates prod resources and access
  labels:
    app: my-java-app
    env: production
    tier: backend
data:
  SPRING_PROFILES_ACTIVE: "prod"               # Activates production Spring profile
  JAVA_OPTS: "-Xms1024m -Xmx1024m -XX:+UseG1GC" # JVM memory/GC config
  LOG_LEVEL: "INFO"                            # Sets application log level
  SERVER_PORT: "8080"                          # Application port
  FEATURE_NEW_PAYMENT_ENABLED: "true"           # Enable/disable payment module
  FEATURE_ADVANCED_REPORTING: "false"           # Enable/disable reporting feature
  SERVICE_TIMEOUT_MS: "5000"                   # API timeout (ms)
  MAX_PAYLOAD_SIZE_MB: "10"                    # Max payload size (MB)
  CACHE_TTL_SECONDS: "3600"                    # Distributed cache TTL (s)
  RETRY_ATTEMPTS: "3"                          # API call retry attempts

---
# Secret: RDS DB credentials and sensitive external keys
apiVersion: v1
kind: Secret
metadata:
  name: prod-app-secrets
  namespace: production
  labels:
    app: my-java-app
    env: production
    tier: backend
type: Opaque
data:
  # RDS Database Connection (Layer 3)
  DB_HOST: "cHJvZC1kYi5hYmMxMjMudXMtZWFzdC0xLnJkcy5hbWF6b25hd3MuY29t"      # "prod-db.abc123.us-east-1.rds.amazonaws.com"
  DB_PORT: "NTQzMg=="                                                       # "5432"
  DB_NAME: "cHJvZHVjdGlvbl9kYXRhYmFzZQ=="                                   # "production_database"
  DB_USERNAME: "cHJvZF9qYXZhX3VzZXI="                                       # "prod_java_user"
  DB_PASSWORD: "ajhAM3g5OUtGJipQYUdkJkZAMTIz"                               # "jH@3x99KF&*PaUd&fD@123"
  # API Keys
  STRIPE_SECRET_KEY: "c2tfbGl2ZV81MUJDRkZCNTJGRjREOUUzQjRCQ0ZGRDU0RkY0REZGRkQ=" # Stripe Live Key
  SENDGRID_API_KEY: "U0cuWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWA==" # SendGrid Key
  # Authentication Secret
  JWT_SECRET: "U3VwZXJTZWN1cmVKd3RTZWNyZXRGb3JQcm9kMjAyNCEkJV4="                # "SuperSecureJwtSecretForProd2024!$%^"


‚Ä¢ What breaks if readinessProbe is wrong?
readinessProbe wrong: Traffic sent to unprepared pods, causing request failures.
livenessProbe wrong: Unnecessary restarts or dead pods not restarted.
Both probes wrong: Crash loops and service outages.
