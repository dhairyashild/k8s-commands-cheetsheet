| Key Point          | Amazon EKS                     | Self-Managed Kubernetes          |
|--------------------|-------------------------------|---------------------------------|
| Management         | 🟢 Fully managed by AWS        | 🟡 User manages everything      |
| Updates & Patching | 🟢 Automatic security patches & upgrades | 🟡 Manual setup required |
| High Availability  | 🟢 Built-in multi-AZ support   | 🟡 Manual AUTOSCALING needed    |
| Control            | 🟡 Less control, more simplicity | 🟢 Full control, more complexity |

🔗 VPC Peering: Creates a direct network connection between two VPCs for private communication without using the internet.

🛠️ kubectl exec: Run commands inside a running container (open shell to debug).
📜 kubectl logs: View logs for troubleshooting output and errors.
🔎 kubectl describe: Get info and events about a resource for debugging issues.

❓ Q: main difference why ecs made if eks there..==ECS was designed to provide simpler container orchestration tightly integrated with AWS and eks for multicloud

📊 Complexity:
- 🟢 ECS is simpler to manage, ideal for AWS-integrated applications.
- 🔵 EKS supports complex microservices architectures with Kubernetes flexibility.

🌍 Portability:
- 🔴 ECS ties workloads to AWS, limiting portability.
- 🟢 EKS offers multi-cloud compatibility via Kubernetes' open standards

❓ Q: how to choose deploy strategy
✅ on company need
🔵🔵 bluegreen & 🐦 canary for 0 downtime but more resources
🔄 rolling for 0 downtime 


 I’ve interviewed 100+ DevOps engineers. Most of them say, “I’ve done K8s in production.”
But when the real questions come in, they freeze.

Here’s what I ask 👇

🔥 1. How does DNS resolution work inside a pod?
→ And what do you check when a service isn’t reachable by name?

🔥 2. Walk me through what the controller manager does during a Deployment.
→ Not rollout status. Reconciliation logic.

🔥 3. What happens if a node with local storage gets autoscaled down?
→ Be careful. This one causes data loss in prod more often than you’d think.

🔥 4. Post-deploy, latency spikes for 30% of users. No errors. No logs. What now?
→ Your answer reveals if you know how to triage chaos.

🔥 5. How do you enforce runtime security in Kubernetes?
→ PSP? AppArmor? OPA? Most people just hope for the best.

🔥 6. HPA vs VPA vs Karpenter — when would you NOT use each?
→ Bonus: How would you simulate HPA behavior in staging?

🔥 7. Tell me about the last outage you debugged in Kubernetes.
→ No postmortem? You weren’t really there.



“A pod can’t resolve service names. DNS looks fine. What’s your next move?”


2. Test DNS Resolution from a Debug Pod
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default
Why?=kubernetes is basic svc always run on k8s, kubernetes.default=kubernetes.default.svc.cluster.local , no need last words if in same ns
Confirms if basic DNS resolution works inside a pod.
If this fails, CoreDNS or networking is misconfigured.

Key Troubleshooting Scenarios
If nslookup kubernetes.default fails but nslookup kubernetes.default.svc.cluster.local works:
→ The pod's search domains in /etc/resolv.conf are misconfigured

If both fail:
→ CoreDNS is down or network policies block DNS
1. Verify CoreDNS 2+ Pods running or not
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns
=Ensures CoreDNS pods are running.
 If not, DNS resolution fails.
k logs and other commands to check , last resort delete coredns deploy & re-install






Interview Cheat Sheet
First Check: nslookup kubernetes.default (basic test).
If Fails check:
1. CoreDNS pods up?
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?

2. kube-dns Service IP correct?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns

3. VPC DNS enabled?
aws ec2 describe-vpcs --vpc-id <VPC-ID> --query "Vpcs[0].[EnableDnsHostnames, EnableDnsSupport]"

Advanced:
1. Check resolv.conf in failing pod. its auto-set in pods
nameserver 10.100.0.10                  # CoreDNS ClusterIP
search namespace.svc.cluster.local ...   # Search domains
options ndots:5                         # Query behavior

Verify no NetworkPolicy blocks DNS.
kubectl get networkpolicies -A	                              #check Blocking UDP 53?

One-Liner to Test Everything:
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default.svc.cluster.local
This covers all critical DNS components in EKS concisely! 


kubeconfig--
Cluster field: Contains Kubernetes API server URL and cluster info.
Context field: Connects user, cluster, and namespace settings.
Users field: Stores authentication details like certificates or tokens.




Deployment strategies you should know as a Devops Engineer:

1. Recreate
- delete all instances then add all instance 
- downtime

2. Rolling update
- replace gradually 
- zero downtime 
- production based app

3. Blue green deployment 
- 2 environments are created
- blue - old instance, green - new instance 
- no downtime 
- high cost
- high level of testing is required

4. Canary deployment 
- 2 environments are created
- a small number of users migrated to a new app 
- no downtime 
- high cost
- high level of testing is required 





• Use case for ConfigMap vs Secret
---
# ConfigMap: Non-sensitive application config for Java Spring Boot backend on AWS EKS
apiVersion: v1
kind: ConfigMap
metadata:
  name: prod-app-config
  namespace: production # Isolates prod resources and access
  labels:
    app: my-java-app
    env: production
    tier: backend
data:
  SPRING_PROFILES_ACTIVE: "prod"               # Activates production Spring profile
  JAVA_OPTS: "-Xms1024m -Xmx1024m -XX:+UseG1GC" # JVM memory/GC config
  LOG_LEVEL: "INFO"                            # Sets application log level
  SERVER_PORT: "8080"                          # Application port
  FEATURE_NEW_PAYMENT_ENABLED: "true"           # Enable/disable payment module
  FEATURE_ADVANCED_REPORTING: "false"           # Enable/disable reporting feature
  SERVICE_TIMEOUT_MS: "5000"                   # API timeout (ms)
  MAX_PAYLOAD_SIZE_MB: "10"                    # Max payload size (MB)
  CACHE_TTL_SECONDS: "3600"                    # Distributed cache TTL (s)
  RETRY_ATTEMPTS: "3"                          # API call retry attempts

---
# Secret: RDS DB credentials and sensitive external keys
apiVersion: v1
kind: Secret
metadata:
  name: prod-app-secrets
  namespace: production
  labels:
    app: my-java-app
    env: production
    tier: backend
type: Opaque
data:
  # RDS Database Connection (Layer 3)
  DB_HOST: "cHJvZC1kYi5hYmMxMjMudXMtZWFzdC0xLnJkcy5hbWF6b25hd3MuY29t"      # "prod-db.abc123.us-east-1.rds.amazonaws.com"
  DB_PORT: "NTQzMg=="                                                       # "5432"
  DB_NAME: "cHJvZHVjdGlvbl9kYXRhYmFzZQ=="                                   # "production_database"
  DB_USERNAME: "cHJvZF9qYXZhX3VzZXI="                                       # "prod_java_user"
  DB_PASSWORD: "ajhAM3g5OUtGJipQYUdkJkZAMTIz"                               # "jH@3x99KF&*PaUd&fD@123"
  # API Keys
  STRIPE_SECRET_KEY: "c2tfbGl2ZV81MUJDRkZCNTJGRjREOUUzQjRCQ0ZGRDU0RkY0REZGRkQ=" # Stripe Live Key
  SENDGRID_API_KEY: "U0cuWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWA==" # SendGrid Key
  # Authentication Secret
  JWT_SECRET: "U3VwZXJTZWN1cmVKd3RTZWNyZXRGb3JQcm9kMjAyNCEkJV4="                # "SuperSecureJwtSecretForProd2024!$%^"


• What breaks if readinessProbe is wrong?
readinessProbe wrong: Traffic sent to unprepared pods, causing request failures.
livenessProbe wrong: Unnecessary restarts or dead pods not restarted.
Both probes wrong: Crash loops and service outages.
