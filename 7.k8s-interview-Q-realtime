Q main difference why ecs made if eks there..==ECS was designed to provide simpler container orchestration tightly integrated with AWS and eks for multicloud
Complexity:

ECS is simpler to manage, ideal for AWS-integrated applications.
EKS supports complex microservices architectures with Kubernetes flexibility.

Portability:
ECS ties workloads to AWS, limiting portability.
EKS offers multi-cloud compatibility via Kubernetes' open standards



Q   how to choose deploy strategy

on company need
bluegreen & canary for 0 downtime but more resources
rolling for 0 downtimetime 


 Iâ€™ve interviewed 100+ DevOps engineers. Most of them say, â€œIâ€™ve done K8s in production.â€
But when the real questions come in, they freeze.

Hereâ€™s what I ask ğŸ‘‡

ğŸ”¥ 1. How does DNS resolution work inside a pod?
â†’ And what do you check when a service isnâ€™t reachable by name?

ğŸ”¥ 2. Walk me through what the controller manager does during a Deployment.
â†’ Not rollout status. Reconciliation logic.

ğŸ”¥ 3. What happens if a node with local storage gets autoscaled down?
â†’ Be careful. This one causes data loss in prod more often than youâ€™d think.

ğŸ”¥ 4. Post-deploy, latency spikes for 30% of users. No errors. No logs. What now?
â†’ Your answer reveals if you know how to triage chaos.

ğŸ”¥ 5. How do you enforce runtime security in Kubernetes?
â†’ PSP? AppArmor? OPA? Most people just hope for the best.

ğŸ”¥ 6. HPA vs VPA vs Karpenter â€” when would you NOT use each?
â†’ Bonus: How would you simulate HPA behavior in staging?

ğŸ”¥ 7. Tell me about the last outage you debugged in Kubernetes.
â†’ No postmortem? You werenâ€™t really there.



â€œA pod canâ€™t resolve service names. DNS looks fine. Whatâ€™s your next move?â€


2. Test DNS Resolution from a Debug Pod
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default
Why?=kubernetes is basic svc always run on k8s, kubernetes.default=kubernetes.default.svc.cluster.local , no need last words if in same ns
Confirms if basic DNS resolution works inside a pod.
If this fails, CoreDNS or networking is misconfigured.

Key Troubleshooting Scenarios
If nslookup kubernetes.default fails but nslookup kubernetes.default.svc.cluster.local works:
â†’ The pod's search domains in /etc/resolv.conf are misconfigured

If both fail:
â†’ CoreDNS is down or network policies block DNS
1. Verify CoreDNS 2+ Pods running or not
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns
=Ensures CoreDNS pods are running.
 If not, DNS resolution fails.
k logs and other commands to check , last resort delete coredns deploy & re-install






Interview Cheat Sheet
First Check: nslookup kubernetes.default (basic test).
If Fails check:
1. CoreDNS pods up?
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?

2. kube-dns Service IP correct?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns

3. VPC DNS enabled?
aws ec2 describe-vpcs --vpc-id <VPC-ID> --query "Vpcs[0].[EnableDnsHostnames, EnableDnsSupport]"

Advanced:
1. Check resolv.conf in failing pod. its auto-set in pods
nameserver 10.100.0.10                  # CoreDNS ClusterIP
search namespace.svc.cluster.local ...   # Search domains
options ndots:5                         # Query behavior

Verify no NetworkPolicy blocks DNS.
kubectl get networkpolicies -A	                              #check Blocking UDP 53?

One-Liner to Test Everything:
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default.svc.cluster.local
This covers all critical DNS components in EKS concisely! 


kubeconfig--

Cluster field: Contains Kubernetes API server URL and cluster info.
Context field: Connects user, cluster, and namespace settings.
Users field: Stores authentication details like certificates or tokens.




Deployment strategies you should know as a Devops Engineer:

1. Recreate
- delete all instances then add all instance 
- downtime

2. Rolling update
- replace gradually 
- zero downtime 
- production based app

3. Blue green deployment 
- 2 environments are created
- blue - old instance, green - new instance 
- no downtime 
- high cost
- high level of testing is required

4. Canary deployment 
- 2 environments are created
- a small number of users migrated to a new app 
- no downtime 
- high cost
- high level of testing is required 

