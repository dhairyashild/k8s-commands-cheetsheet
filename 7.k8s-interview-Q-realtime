| Key Point          | Amazon EKS                     | Self-Managed Kubernetes          |
|--------------------|-------------------------------|---------------------------------|
| Management         | ğŸŸ¢ Fully managed by AWS        | ğŸŸ¡ User manages everything      |
| Updates & Patching | ğŸŸ¢ Automatic security patches & upgrades | ğŸŸ¡ Manual setup required |
| High Availability  | ğŸŸ¢ Built-in multi-AZ support   | ğŸŸ¡ Manual AUTOSCALING needed    |
| Control            | ğŸŸ¡ Less control, more simplicity | ğŸŸ¢ Full control, more complexity |

ğŸ”— VPC Peering: Creates a direct network connection between two VPCs for private communication without using the internet.

ğŸ› ï¸ kubectl exec: Run commands inside a running container (open shell to debug).
ğŸ“œ kubectl logs: View logs for troubleshooting output and errors.
ğŸ” kubectl describe: Get info and events about a resource for debugging issues.

â“ Q: main difference why ecs made if eks there..==ECS was designed to provide simpler container orchestration tightly integrated with AWS and eks for multicloud

ğŸ“Š Complexity:
- ğŸŸ¢ ECS is simpler to manage, ideal for AWS-integrated applications.
- ğŸ”µ EKS supports complex microservices architectures with Kubernetes flexibility.

ğŸŒ Portability:
- ğŸ”´ ECS ties workloads to AWS, limiting portability.
- ğŸŸ¢ EKS offers multi-cloud compatibility via Kubernetes' open standards

â“ Q: how to choose deploy strategy
âœ… on company need
ğŸ”µğŸ”µ bluegreen & ğŸ¦ canary for 0 downtime but more resources
ğŸ”„ rolling for 0 downtime 


 Iâ€™ve interviewed 100+ DevOps engineers. Most of them say, â€œIâ€™ve done K8s in production.â€
But when the real questions come in, they freeze.

Hereâ€™s what I ask ğŸ‘‡

ğŸ”¥ 1. How does DNS resolution work inside a pod?
â†’ And what do you check when a service isnâ€™t reachable by name?

ğŸ”¥ 2. Walk me through what the controller manager does during a Deployment.
â†’ Not rollout status. Reconciliation logic.

ğŸ”¥ 3. What happens if a node with local storage gets autoscaled down?
â†’ Be careful. This one causes data loss in prod more often than youâ€™d think.

ğŸ”¥ 4. Post-deploy, latency spikes for 30% of users. No errors. No logs. What now?
â†’ Your answer reveals if you know how to triage chaos.

ğŸ”¥ 5. How do you enforce runtime security in Kubernetes?
â†’ PSP? AppArmor? OPA? Most people just hope for the best.

ğŸ”¥ 6. HPA vs VPA vs Karpenter â€” when would you NOT use each?
â†’ Bonus: How would you simulate HPA behavior in staging?

ğŸ”¥ 7. Tell me about the last outage you debugged in Kubernetes.
â†’ No postmortem? You werenâ€™t really there.



â€œA pod canâ€™t resolve service names. DNS looks fine. Whatâ€™s your next move?â€


2. Test DNS Resolution from a Debug Pod
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default
Why?=kubernetes is basic svc always run on k8s, kubernetes.default=kubernetes.default.svc.cluster.local , no need last words if in same ns
Confirms if basic DNS resolution works inside a pod.
If this fails, CoreDNS or networking is misconfigured.

Key Troubleshooting Scenarios
If nslookup kubernetes.default fails but nslookup kubernetes.default.svc.cluster.local works:
â†’ The pod's search domains in /etc/resolv.conf are misconfigured

If both fail:
â†’ CoreDNS is down or network policies block DNS
1. Verify CoreDNS 2+ Pods running or not
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns
=Ensures CoreDNS pods are running.
 If not, DNS resolution fails.
k logs and other commands to check , last resort delete coredns deploy & re-install






Interview Cheat Sheet
First Check: nslookup kubernetes.default (basic test).
If Fails check:
1. CoreDNS pods up?
kubectl get pods -n kube-system -l k8s-app=kube-dns  # CoreDNS running?

2. kube-dns Service IP correct?
kubectl get svc -n kube-system kube-dns              # ClusterIP svc for above coredns-pod= kube-dns

3. VPC DNS enabled?
aws ec2 describe-vpcs --vpc-id <VPC-ID> --query "Vpcs[0].[EnableDnsHostnames, EnableDnsSupport]"

Advanced:
1. Check resolv.conf in failing pod. its auto-set in pods
nameserver 10.100.0.10                  # CoreDNS ClusterIP
search namespace.svc.cluster.local ...   # Search domains
options ndots:5                         # Query behavior

Verify no NetworkPolicy blocks DNS.
kubectl get networkpolicies -A	                              #check Blocking UDP 53?

One-Liner to Test Everything:
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never -- nslookup kubernetes.default.svc.cluster.local
This covers all critical DNS components in EKS concisely! 


kubeconfig--
Cluster field: Contains Kubernetes API server URL and cluster info.
Context field: Connects user, cluster, and namespace settings.
Users field: Stores authentication details like certificates or tokens.




Deployment strategies you should know as a Devops Engineer:

1. Recreate
- delete all instances then add all instance 
- downtime

2. Rolling update
- replace gradually 
- zero downtime 
- production based app

3. Blue green deployment 
- 2 environments are created
- blue - old instance, green - new instance 
- no downtime 
- high cost
- high level of testing is required

4. Canary deployment 
- 2 environments are created
- a small number of users migrated to a new app 
- no downtime 
- high cost
- high level of testing is required 





â€¢ Use case for ConfigMap vs Secret
---
# ConfigMap: Non-sensitive application config for Java Spring Boot backend on AWS EKS
apiVersion: v1
kind: ConfigMap
metadata:
  name: prod-app-config
  namespace: production # Isolates prod resources and access
  labels:
    app: my-java-app
    env: production
    tier: backend
data:
  SPRING_PROFILES_ACTIVE: "prod"               # Activates production Spring profile
  JAVA_OPTS: "-Xms1024m -Xmx1024m -XX:+UseG1GC" # JVM memory/GC config
  LOG_LEVEL: "INFO"                            # Sets application log level
  SERVER_PORT: "8080"                          # Application port
  FEATURE_NEW_PAYMENT_ENABLED: "true"           # Enable/disable payment module
  FEATURE_ADVANCED_REPORTING: "false"           # Enable/disable reporting feature
  SERVICE_TIMEOUT_MS: "5000"                   # API timeout (ms)
  MAX_PAYLOAD_SIZE_MB: "10"                    # Max payload size (MB)
  CACHE_TTL_SECONDS: "3600"                    # Distributed cache TTL (s)
  RETRY_ATTEMPTS: "3"                          # API call retry attempts

---
# Secret: RDS DB credentials and sensitive external keys
apiVersion: v1
kind: Secret
metadata:
  name: prod-app-secrets
  namespace: production
  labels:
    app: my-java-app
    env: production
    tier: backend
type: Opaque
data:
  # RDS Database Connection (Layer 3)
  DB_HOST: "cHJvZC1kYi5hYmMxMjMudXMtZWFzdC0xLnJkcy5hbWF6b25hd3MuY29t"      # "prod-db.abc123.us-east-1.rds.amazonaws.com"
  DB_PORT: "NTQzMg=="                                                       # "5432"
  DB_NAME: "cHJvZHVjdGlvbl9kYXRhYmFzZQ=="                                   # "production_database"
  DB_USERNAME: "cHJvZF9qYXZhX3VzZXI="                                       # "prod_java_user"
  DB_PASSWORD: "ajhAM3g5OUtGJipQYUdkJkZAMTIz"                               # "jH@3x99KF&*PaUd&fD@123"
  # API Keys
  STRIPE_SECRET_KEY: "c2tfbGl2ZV81MUJDRkZCNTJGRjREOUUzQjRCQ0ZGRDU0RkY0REZGRkQ=" # Stripe Live Key
  SENDGRID_API_KEY: "U0cuWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWA==" # SendGrid Key
  # Authentication Secret
  JWT_SECRET: "U3VwZXJTZWN1cmVKd3RTZWNyZXRGb3JQcm9kMjAyNCEkJV4="                # "SuperSecureJwtSecretForProd2024!$%^"


â€¢ What breaks if readinessProbe is wrong?
readinessProbe wrong: Traffic sent to unprepared pods, causing request failures.
livenessProbe wrong: Unnecessary restarts or dead pods not restarted.
Both probes wrong: Crash loops and service outages.
